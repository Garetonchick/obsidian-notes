$$
a(x) = \langle w, x\rangle + w_0\quad w\in \mathbb{R}^d, w_0\in\mathbb{R}
$$
## Функционалы ошибки
Часто используют MSE(Mean Squared Error)
$$
Q(X, y, a) = \frac{1}{n}
\sum_{i=1}^n(y_i-a(X_i))^2
$$
Для MSE существует аналитическое решение. Тут мы считаем, что у нас есть константный признак для каждого объекта, которому и соответствует $w_0$.
$$
w^{\ast} = (X^TX)^{-1}X^Ty
$$
В более общем случае, если Q дифференцируема, то 
можно воспользоваться [[Градиентный спуск|градиентным спуском]], чтобы найти решение численно.
## Регуляризация
Регуляризация &mdash; добавление некоторого штрафа в модель за большие веса в модели. Используется для предотвращения переобучения.
### L2 регуляризация = Ridge regression
$$Q_{reg}(X, y, a) = Q(X, y, a) + \lambda \|w\|_2^2$$
В случае MSE есть аналитическое решение:
$$
w^{\ast} = (X^TX + \lambda I)^{-1}X^Ty
$$
### L1 регуляризация = Lasso regression
$$
Q_{reg}(X, y, a) = Q(X, y, a) + \lambda \|w\|_1
$$
Помогает занулять некоторые веса тем самым избавляясь от лишних признаков.

**Замечание 1:** Нельзя использовать вес $w_0$ в регуляризации т. к. изменение признаков никак не влияет на вклад $w_0$ в ответ модели.
**Замечание 2:** $\lambda$ - гиперпараметр, его нельзя подбирать по тренировчной или по тестовой выборке.
## Корреляция Пирсона
$$
\text{corr}(X, Y)=\frac{\operatorname{cov}_{X Y}}{\sigma_X \sigma_Y}=\frac{\sum_{i=1}^n(X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum_{i=1}^n(X_i - \bar{X})^2 \sum_{i=1}^n(Y_i - \bar{Y})^2}}
$$
- Чем больше по модулю, тем более  "зависимы" X и Y. 
- Случай $|\text{corr}(X, Y)|=1$ означает линейную зависимость X и Y.
- Сильно зависимые признаки приводят к переобучению.
## Выбросы
При использовании MSE в качестве функции ошибки очень важно убирать выбросы из выборки т. к. они слишком сильно влияют на значение MSE.
## Кросс-валидация
Делим выборку на $k$ частей. Перебираем часть для теста, остальное используем для обучения. Чтобы сделать предсказание на новых данных берем среднее предсказаний каждой модели.
<img src="https://docs.splunk.com/images/thumb/e/ee/Kfold_cv_diagram.png/1200px-Kfold_cv_diagram.png" width=100%>
**Плюс:** Более эффективно используем выборку, не нужно выделять отдельный кусок для валидации.

**Минус:** Нужно учить $k$ моделей. В случае линейной регрессии это обычно приемлемо. 
## Масштабирование признаков
### Стандартизация
Стандартизация &mdash; приведение распределения признака к распределению c нулевым матожиданием и единичной дисперсией.
$$
x_i' = \frac{x_i - \mu_i}{\sigma_i}
$$
### Нормализация
Перевод всех значений в отрезок $[-1, 1]$. Пусть минимальное значение признака $i$ это $a_i$ и максимальное $b_i$, тогда:
$$
x_i' = \frac{x_i-\mu_i}{b_i-a_i}
$$
**Замечание:** на практике чаще используют стандартизацию.
